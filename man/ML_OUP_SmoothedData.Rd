% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MaximumLikelihoodHertz.R
\docType{data}
\name{ML_OUP_SmoothedData}
\alias{ML_OUP_SmoothedData}
\title{Smoothed sample paths for the Ornstein-Uhlenbeck Process}
\format{
csv file with 170 rows and 10 columns
}
\description{
This is not even a simulated data set. Instead a simulated sample path is
smoothed nine times by estimating the parameters and calculating the means
for each observation. Then the calculated means are used, as if they are
data, in a subsequent estimation. Means are calculated again and used in
the next estimation and so on nine times. The rate of convergence and the
location parameters are recovered from each estimation, but the scale
parameter gets successively smaller.
}
\details{
By the ninth smoothing, the log of the likelihood becomes positive. Hence,
the likelihood, itself, becomes greater than one. In other words, if this
were a real sample path, there would be a greater than 100\% chance it would
be observed. A small sigma or a positive log likelihood are tell-tale signs
the data has been smoothed.

This is the second best scenario. The model used for the smoothing is
consistent with the data. Surprisingly, hypothesis tests and decision
thresholds are not greatly affected.

The third best scenario is much more likely. The data may have been smoothed
by an undocumented model inconsistent with the model you wish to estimate.
Your estimates will be rubbish but you won't be able to tell.

The first best scenario is finding the raw data.

\itemize{
\item year: time variable in annual increments
\item Data: simulated sample path
\item G uno-G nuevo: nine successively smoother paths
}
}
\keyword{datasets}
